# ðŸ“˜ Day-3 Teaching Notebook â€” RAG Code Revision (Role Model)

## ðŸ§­ Pre-Flight â€” Repo & Run Map

```text
day3_rag_project/
|-- app/
|   \-- app.py              # UI (Gradio/Streamlit) â†’ calls pipeline
|-- pipeline/
|   |-- ingest.py           # ðŸ”´ BIT-1: Chunk docs â†’ embed â†’ store vectors
|   |-- retrieve.py         # ðŸŸ¡ BIT-2: Retriever (filters, timeouts)
|   |-- rerank.py           # ðŸŸ¡ BIT-3: Cross-encoder reranker
|   |-- answer.py           # ðŸ”´ BIT-4: Cited answer builder
|   \-- cache.py            # ðŸŸ¢ BIT-5: Query cache (disk/Redis)
|-- eval/
|   |-- dataset.jsonl       # Eval set
|   |-- harness.py          # Eval harness
|   |-- ab_test.py          # ðŸŸ¡ BIT-8: Compare rerankers
|   |-- expansion.py        # ðŸŸ¢ BIT-6: Query expansion
|   \-- reports/            # Reports
|-- configs/
|   \-- rag.yaml            # Config knobs
\-- utils/
    |-- timer.py            # ðŸ”µ BIT-9: Latency budgeting
    \-- safe_call.py        # ðŸ”µ BIT-10: Graceful degradation
```

---

## ðŸ”´ BIT-1 â€” Deterministic Chunking (Heart)

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(   # ðŸ”‘ chunk control
    chunk_size=800,
    chunk_overlap=120,
    separators=["\n\n", ".", "?", "!"]
)
chunks = splitter.split_text(doc)  # â†’ List[str]
```

- ðŸŸ¢ Dummy-Friendly: Like slicing bread into equal slices with overlaps.
- ðŸ”µ Deep Dive: Input raw doc â†’ output chunks. Deterministic. Params: 800 size, 120 stride.
- âœ… Guarantees: consistent splits
- âš ï¸ Breaks if: doc > model max token

```text
[Docs] -> [Chunker â€” BIT-1] -> [Chunks[]]
```

---

## ðŸŸ¡ BIT-2 â€” Retriever with Filters & Timeouts (Ribs)

```python
hits = store.similarity_search(
    query_vec,
    k=10,                          # ðŸ”‘ top-k knob
    filter={"source": "docs"},
    timeout=1.5
)
```

- ðŸŸ¢ Dummy-Friendly: Like a librarian told: â€œonly from history section, back in 2 min.â€
- ðŸ”µ Deep Dive: Vector sim (FAISS/Chroma), filters prune irrelevant docs, timeout avoids hangs.
- âœ… Guarantees: fast, scoped results
- âš ï¸ Breaks if: timeout too strict -> empty set

```text
[Query Vec] -> [Retriever â€” BIT-2] -> [Top-k Docs]
```

---

## ðŸŸ¡ BIT-3 â€” Cross-Encoder Reranker (Ribs -> Heart)

```python
pairs = [(q, d) for d in hits]
scores = cross_encoder.predict(pairs)   # ðŸ”‘ scoring loop
reranked = sorted(zip(hits, scores), key=lambda x: x[1], reverse=True)
```

- ðŸŸ¢ Dummy-Friendly: Librarian reads blurbs to pick best book.
- ðŸ”µ Deep Dive: Input query+doc pairs, outputs ranked list. Model: MiniLM cross-encoder.
- âœ… Guarantees: better precision
- âš ï¸ Breaks if: too slow -> blows latency budget

```text
[Top-k Docs] + [Query] -> [Cross-Encoder â€” BIT-3] -> [Ranked Docs]
```

---

## ðŸ”´ BIT-4 â€” Cited Answer Builder (Heart)

```python
context = "\n".join(d.page_content for d in top_docs[:4])  # ðŸ”‘ context window
prompt = f"Q: {query}\n\nContext:\n{context}\n\nA:"
answer = llm.generate(prompt)
```

- ðŸŸ¢ Dummy-Friendly: Writer builds an answer with footnotes.
- ðŸ”µ Deep Dive: Context top 4 docs, citations enforced, fallback 'I don't know'.
- âœ… Guarantees: grounded answers
- âš ï¸ Breaks if: context > model token limit

```text
[Ranked Docs] -> [Answer Builder â€” BIT-4] -> [Cited Answer]
```

---

## ðŸŸ¢ BIT-5 â€” Lightweight Cache (Hooks)

```python
key = (query, cfg_hash)   # ðŸ”‘ key design
if key in cache:
    return cache[key]
else:
    result = run_pipeline(query)
    cache[key] = result
```

- ðŸŸ¢ Dummy-Friendly: Sticky notes â†’ if asked yesterday, answer is ready today.
- ðŸ”µ Deep Dive: Cache key = query + config, backends disk or Redis.
- âœ… Guarantees: speed + cost saving
- âš ï¸ Breaks if: config changes but cache not invalidated

```text
[Query] -> [Cache â€” BIT-5] -> [Answer / Pipeline]
```

---

## ðŸŸ¢ BIT-6 â€” Query Expansion (Hooks)

```python
if top_docs:   # ðŸ”‘ PRF branch
    pseudo_ctx = "\n".join(d.text for d in top_docs[:3])
    prompt = f"Expand '{q}' using:\n{pseudo_ctx}"
else:          # HyDE branch
    prompt = f"Generate hypothetical passage for: {q}"
return llm.generate(prompt).text
```

- ðŸŸ¢ Dummy-Friendly: Friend rephrases your vague question.
- ðŸ”µ Deep Dive: PRF reformulates using top docs, HyDE hallucinates fake passage.
- âœ… Guarantees: better recall
- âš ï¸ Breaks if: expansion adds noise

```text
[Query] -> [Expansion â€” BIT-6] -> [Expanded Query] -> Retriever
```

---

## ðŸŸ¡ BIT-7 â€” Negative Sampling for Eval

```python
negs = store.sample_random(n=2)  # ðŸ”‘ random distractors
ex["candidates"] = ex["references"] + [d.id for d in negs]
```

- ðŸŸ¢ Dummy-Friendly: Teacher adds trick answers in a quiz.
- ðŸ”µ Deep Dive: Augments eval set w/ distractors, tests rerankerâ€™s discrimination.
- âœ… Guarantees: fair eval
- âš ï¸ Breaks if: negatives too easy/hard

```text
[Eval Set] -> [Add Negatives â€” BIT-7] -> [Augmented Eval Set]
```

---

## ðŸŸ¡ BIT-8 â€” AB Test Rerankers

```python
for name, rer in rerankers.items():   # ðŸ”‘ loop compare
    metrics = [score(rer.rerank(q, retriever.query(q)), q) for q in qs]
    results[name] = sum(metrics) / len(metrics)
```

- ðŸŸ¢ Dummy-Friendly: Taste-test 2 chefs â†’ pick best dish.
- ðŸ”µ Deep Dive: Inputs: queries + reranker models, outputs average metrics per reranker.
- âœ… Guarantees: evidence-based choice
- âš ï¸ Breaks if: sample size too small

```text
[Queries] -> [Reranker A â€” BIT-8] -> [Metrics A]
[Queries] -> [Reranker B â€” BIT-8] -> [Metrics B]
```

---

## ðŸ”µ BIT-9 â€” Latency Budgeting

```python
@contextmanager
def timer(label):    # ðŸ”‘ p95 tracker
    t0 = time.time()
    yield
    print(label, "took", time.time()-t0, "s")
```

- ðŸŸ¢ Dummy-Friendly: Stopwatch for each runner in a relay.
- ðŸ”µ Deep Dive: Wrap blocks in timers, SLA p95 < 3.5s.
- âœ… Guarantees: SLA compliance
- âš ï¸ Breaks if: only average tracked (hides spikes)

```text
[Pipeline Step] -> [Timer â€” BIT-9] -> [Latency Report]
```

---

## ðŸ”µ BIT-10 â€” Error Handling & Graceful Degrade

```python
def safe_call(fn, fallback):   # ðŸ”‘ fallback guard
    try:
        return fn()
    except Exception:
        return fallback
```

- ðŸŸ¢ Dummy-Friendly: Backup generator keeps lights on.
- ðŸ”µ Deep Dive: Wrap risky calls, fallback local model/retriever, prevents crash.
- âœ… Guarantees: no full crash
- âš ï¸ Breaks if: fallback too weak

```text
[Function] -> [SafeCall â€” BIT-10] -> [Result/Fallback]
```

---

## ðŸ”¬ Quick Eval Harness (Smoke)

```python
def run_smoke_test(n=3, cfg="configs/rag.yaml"):
    dataset = load_dataset("eval/dataset.jsonl")[:n]
    results = []
    for ex in dataset:
        ans = pipeline.ask(ex["question"], cfg)
        results.append(score(ans, ex["references"]))
    return report(results)
```

```text
[Mini Eval] -> [Pipeline] -> [Scoring EM/F1] -> [Report HTML/JSON]
```

---

## âš ï¸ Pitfalls & Debugging Checklist

- Vector dimension mismatch -> FAISS crash
- Timeout too strict -> empty retrievals
- Cache key mismatch -> stale answers
- Context overflow -> silent truncation
- Latency untracked -> SLA violations
- No negatives -> misleading eval

---

## ðŸš€ End-to-End Pipeline ASCII

```text
Docs -> [BIT-1 Chunker] -> [Embeddings] -> [BIT-2 Retriever] -> [BIT-3 Reranker]
     -> [BIT-4 Answer Builder] -> [BIT-5 Cache]
                 -> [BIT-6 Expansion] -> [Retriever]

Eval: [BIT-7 Negatives] + [BIT-8 AB Test] + [BIT-9 Timer] + [BIT-10 SafeCall]
```

---

## ðŸ“‘ Quick Sheet (1-Page Revision)

```text
BIT-1: Chunker -> consistent slices
BIT-2: Retriever -> fast, filtered
BIT-3: Reranker -> improves precision
BIT-4: Answer -> citations, fallback
BIT-5: Cache -> saves cost/latency
BIT-6: Expansion -> better recall
BIT-7: Negatives -> fair eval
BIT-8: AB test -> pick best reranker
BIT-9: Timer -> SLA tracking
BIT-10: SafeCall -> resilience
```

**Top 5 Interview Qs**
1. Why chunk docs?
2. FAISS vs Chroma?
3. Why rerankers?
4. Why negatives in eval?
5. What is graceful degrade?
